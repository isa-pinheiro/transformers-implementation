{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObxHDXvXAQYzbrp6NpfG6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isa-pinheiro/transformers-implementation/blob/main/transformers_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Y9hF-hk6sSee"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to implement\n",
        "1. encoder\n",
        "2. decoder\n",
        "\n",
        "details\n",
        "1. word embedding\n",
        "2. positional encoding\n",
        "3. multiheaded attention\n",
        "4. feed foward\n",
        "5. cross attention\n",
        "\n",
        "block\n",
        "1. multihead attention + add & norm\n",
        "2. masked multihead attention + add & norm\n",
        "3. feedfoward + add & norm"
      ],
      "metadata": {
        "id": "yQl1oGQAsnw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout\n",
        "- aplicado no final de cada sublayer (antes de ser adicionada ao input e normalizado)\n",
        "- adicionado ao somatório do embedding e do positional encoding"
      ],
      "metadata": {
        "id": "_rIes3V7VN_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        # d_model tamanho do vetor para cada palavra após o embedding\n",
        "        # vocab_size tokens com o dataset de sentence pairs\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # !!não aceita diretamente texto, precisa de que seja transfromado em valores numéricos\n",
        "        x = x *  np.sqrt(self.d_model)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hrFLwYoyAlj6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, seq_len, dropout = 0.1):\n",
        "        # d_model é o tamanho do vetor para cada ser somado a palavra (tamanho embedding)\n",
        "            # attention is all you need = 512\n",
        "        # seq_len é o tamanho da sentença do input\n",
        "            # attention is all you need usou sentence pairs de aproximadamente 25k tokens\n",
        "        # dropout é a regularização usada\n",
        "            # p = 0.1 no attention is all you need\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.empty(seq_len, d_model)\n",
        "\n",
        "        # pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # pos_exp = pos * torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        pos_exp = pos / torch.pow(10000, 2 * torch.arange(0, d_model, 2) / d_model) # (seq_len, d_model)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(pos_exp) # pega somente\n",
        "        pe[:, 1::2] = torch.cos(pos_exp)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x + (self.pe[:, :x.shape[1]])\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yL77otwLb5Tw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separa as informações em heads de forma que cada parte seja processada individualmente e aprenda informações diferentes\n",
        "# precisa -> dimensão do modelo (quantas tem no embedding)\n",
        "#         -> quantidade de heads (deve ser um dos divisores da dimensão do modelo)\n",
        "#         -> matrizes para encontrar o query, key e value (precisa de uma camada linear)\n",
        "#         -> saída é uma camada linear també m\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "\n",
        "        if self.d_model // self.heads == self.d_model / self.heads:\n",
        "            self.d_head = self.d_model // self.heads\n",
        "        else:\n",
        "            print('A dimensão do modelo deve ser divisível pela quantidade de heads!')\n",
        "\n",
        "        # pesos para definir query, key e value\n",
        "        self.wquery = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.wkey = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.wvalue = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.woutput = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def self_attention(query, key, value, mask, dropout):\n",
        "        d_head = query.shape[-1]\n",
        "\n",
        "        score_matrix = (query @ key.transpose(-2, -1)) / np.sqrt(d_head) # define os score de atenção, pelas interações de query e key\n",
        "                                                                        # relação de palavra por palavra (batch, head, seq_len, seq_len)\n",
        "        if mask:\n",
        "            score_matrix.masked_fill_(mask == 0, -torch.inf)\n",
        "        score_matrix = score_matrix.softmax(dim=-1) # (batch, head, seq_len, seq_len)\n",
        "\n",
        "        if dropout:\n",
        "            score_matrix = dropout(score_matrix)\n",
        "\n",
        "        final_score_matrix = score_matrix @ value\n",
        "\n",
        "        return final_score_matrix, score_matrix\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        # q é a entrada de dimensão (batch, seq_len, d_model)\n",
        "        #\n",
        "        query = self.wquery(q) # (batch, seq_len, d_model)\n",
        "        key = self.wkey(k)\n",
        "        value = self.wvalue(v)\n",
        "\n",
        "        query = query.view(query.shape[0], query.shape[1], self.heads, self.d_head) # mantem as duas dimensões iniciais e divide a última\n",
        "        key = key.view(key.shape[0], key.shape[1], self.heads, self.d_head)         # com a quantidade de heads e da dimensão das heads\n",
        "        value = value.view(value.shape[0], value.shape[1], self.heads, self.d_head) # (batch, seq_len, heads, d_head)\n",
        "\n",
        "        query = query.transpose(1,2)    # troca o heads com o seq_len\n",
        "        key = key.transpose(1,2)        # (batch, heads, seq_len, d_head)\n",
        "        value = value.transpose(1,2)    # tem toda a frase, mas apenas parte do embedding. só aprende com parte do embedding\n",
        "\n",
        "        x, self.score_matrix = MultiHeadedAttention.self_attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2) # retorna heads e o seq_len para as posições iniciais (batch, seq_len, heads, d_head)\n",
        "        x = x.contiguous()\n",
        "        x = x.view(x.shape[0], -1, self.heads * self.d_head)\n",
        "\n",
        "        x = self.woutput(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "YoILoddly3cf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pointwise feedfoward\n",
        "# rede feed foward completamente conectada\n",
        "    # duas transformações lineares com uma relu no meio\n",
        "# aplica dropout - 0.1\n",
        "# FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "class PointwiseFeedFoward(nn.Module):\n",
        "    def __init__(self, d_model, dff, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dff) # W1, b1\n",
        "        self.linear2 = nn.Linear(dff, d_model) # W2, b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gl_v9Q6sGKYw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# norm_xj = (xj  - mean_j / (sqrt(sqrd(std) + epsilon))) * gamma + beta ; j varia com os batches\n",
        "# epsilon é usado para estabilizar a normalização, valores não explodirem se o desvio padrão for muito grande\n",
        "# gamma e beta são valores aprendidos com a backpropagation\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, epsilon = 1e-05):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = nn.Parameter(torch.ones(1))\n",
        "        self.beta = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim=True)\n",
        "        std = x.std(dim = -1, keepdim=True)\n",
        "\n",
        "        x = ((x - mean) / (std + self.epsilon)) * self.gamma + self.beta\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "uBmp4aESkjJM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5cLRIGvWSK9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff = 64\n",
        "heads = 4\n",
        "d_model = 16\n",
        "seq_len = 10\n",
        "vocab_size = 100\n",
        "\n",
        "x = torch.randint(0, vocab_size, (1, seq_len))\n",
        "print(\"input:\", x[0, 9])\n",
        "\n",
        "embedding_layer = InputEmbedding(d_model, vocab_size)\n",
        "x_input_embedded = embedding_layer(x)\n",
        "print(\"Saída input embeding: \\n \", x_input_embedded[0, 9])\n",
        "\n",
        "pos_encoding = PositionalEncoding(d_model, seq_len, 0.1)\n",
        "x_pos_encoded = pos_encoding(x_input_embedded)\n",
        "print(\"Saída positional encoding: \\n\", x_pos_encoded[0, 9])\n",
        "\n",
        "multiheads = MultiHeadedAttention(d_model, heads)\n",
        "x_multiheads = multiheads(x_pos_encoded, x_pos_encoded, x_pos_encoded, mask=None)\n",
        "print(\"Saída MultiHeadedAttention: \\n\", x_multiheads[0, 9])\n",
        "\n",
        "layernorm = LayerNorm()\n",
        "x_norm = layernorm(x_multiheads)\n",
        "print(\"Saída layernorm: \\n\", x_norm[0, 9])\n",
        "\n",
        "feedforward = PointwiseFeedFoward(d_model, dff)\n",
        "x_feedforward = feedforward(x_norm)\n",
        "print(\"Saída feedforward: \\n\", x_feedforward[0, 9])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCq4LYBjZBy9",
        "outputId": "1c14abe3-8e55-417e-a78b-fa2e63e8ebcc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor(34)\n",
            "Saída input embeding: \n",
            "  tensor([-5.5695, -4.6235,  0.2193, -2.8782, -1.5296,  1.8163, -3.3187, -0.2757,\n",
            "        -1.9703, -3.2695, -5.4197, -1.3704, -0.9696, -3.6081, -0.0599, -5.7804],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Saída positional encoding: \n",
            " tensor([-5.7305, -6.1496,  1.1141, -2.5073, -1.5997,  3.1248, -3.6774,  0.8047,\n",
            "        -2.1883, -0.0000, -6.0218, -0.4116, -1.0773, -0.0000, -0.0666, -5.3115],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Saída MultiHeadedAttention: \n",
            " tensor([ 1.1758,  0.6210,  0.8832,  0.7498, -0.6326, -0.5076,  0.2836,  0.7983,\n",
            "        -1.2233, -0.6680, -0.2716, -0.0599,  0.4851,  0.5932,  0.1017, -0.3571],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Saída layernorm: \n",
            " tensor([ 1.5422,  0.7293,  1.1134,  0.9179, -1.1073, -0.9242,  0.2350,  0.9890,\n",
            "        -1.9728, -1.1592, -0.5785, -0.2683,  0.5302,  0.6886, -0.0316, -0.7037],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Saída feedforward: \n",
            " tensor([-0.0976,  0.0638,  0.1878, -0.3013,  0.2010, -0.1381,  0.7919,  0.1216,\n",
            "         0.4522,  0.1249, -0.1378,  0.0976, -0.3863,  0.2234, -0.1481,  0.2687],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "print(x.shape)\n",
        "print(x[0,0])\n",
        "print(embedding.weight[x[0, 0]])\n",
        "print(embedding(x)[0,0])"
      ],
      "metadata": {
        "id": "c9z22ZS_iNoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e5a7f9-b503-4a83-c621-946f959bea4a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n",
            "tensor(17)\n",
            "tensor([ 0.1304,  0.6675, -0.4013, -1.3805, -0.3103,  0.3276, -1.5100, -1.6990,\n",
            "        -1.0452, -0.9526,  1.2100, -0.7109,  0.6958, -0.5963,  0.2851,  0.5929],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([ 0.1304,  0.6675, -0.4013, -1.3805, -0.3103,  0.3276, -1.5100, -1.6990,\n",
            "        -1.0452, -0.9526,  1.2100, -0.7109,  0.6958, -0.5963,  0.2851,  0.5929],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    }
  ]
}